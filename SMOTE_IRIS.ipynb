{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83923442-a9cf-4dcb-a466-a2cc5fe7d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title & Notes\n",
    "# ----------------------\n",
    "# Scaling the Iris dataset with SMOTE (configurable for huge expansion on Nautilus)\n",
    "# Instructions:\n",
    "#   - By default this notebook runs in TEST MODE to avoid overloading local machines.\n",
    "#   - When running on Nautilus: set small_test_mode = False, update output_dir to your PVC mount,\n",
    "#     ensure required packages are installed in the container, and adjust resources.\n",
    "# Paths: use /mnt/data for PVC-mounted storage on Nautilus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd196dfa-e2b0-40de-b956-3f5b216949aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with multiplier: 10\n",
      "Output directory: /home/jovyan/cloud_comp/module5/practices/project1\n"
     ]
    }
   ],
   "source": [
    "# Parameters (edit before running at scale)\n",
    "small_test_mode = True        # <-- Set to False on Nautilus when ready to scale\n",
    "original_multiplier = 100000  # intended multiplier (100k)\n",
    "test_multiplier = 10          # small multiplier for local testing\n",
    "output_dir = \"/home/jovyan/cloud_comp/module5/practices/project1\"  # change to your PVC mount on Nautilus\n",
    "\n",
    "# Effective multiplier used by the notebook\n",
    "multiplier = test_multiplier if small_test_mode else original_multiplier\n",
    "\n",
    "print(\"Running with multiplier:\", multiplier)\n",
    "print(\"Output directory:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "113556c8-f74b-4422-9dd4-bd24362010e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing: ['scikit-learn']\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 — Install dependencies (run once)\n",
    "# Note: On Nautilus you may prefer to build a Docker image with these already installed.\n",
    "import sys, subprocess, pkgutil\n",
    "def pip_install(pkgs):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *pkgs])\n",
    "\n",
    "needed = []\n",
    "for pkg in (\"pandas\",\"numpy\",\"scikit-learn\",\"joblib\",\"pyarrow\"):\n",
    "    if not pkgutil.find_loader(pkg):\n",
    "        needed.append(pkg)\n",
    "# imbalanced-learn is optional; will be installed if needed\n",
    "if needed:\n",
    "    print(\"Installing:\", needed)\n",
    "    pip_install(needed)\n",
    "else:\n",
    "    print(\"Core packages present. If you need imbalanced-learn later, install it then.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37717c54-6d56-476a-b0e0-1eb880bc197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory ready: /home/jovyan/cloud_comp/module5/practices/project1\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — Imports and setup\n",
    "import os, math, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import joblib\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(\"Output directory ready:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27167d65-c983-475d-bd54-d2decf7166a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (150, 4)\n",
      "species\n",
      "0    50\n",
      "1    50\n",
      "2    50\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length  sepal width  petal length  petal width\n",
       "0           5.1          3.5           1.4          0.2\n",
       "1           4.9          3.0           1.4          0.2\n",
       "2           4.7          3.2           1.3          0.2\n",
       "3           4.6          3.1           1.5          0.2\n",
       "4           5.0          3.6           1.4          0.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 5 — Load Iris (built-in) and inspect\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=[c.replace(\" (cm)\",\"\").strip() for c in iris.feature_names])\n",
    "y = pd.Series(iris.target, name=\"species\")\n",
    "print(\"Original shape:\", X.shape)\n",
    "print(y.value_counts())\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6817d268-fabf-4931-b204-97e9ee45a0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (120, 4) Test shape: (30, 4)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 — Preprocessing: scaling and train-test split BEFORE augmentation\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "scaler_params = pd.DataFrame({\n",
    "    \"mean\": scaler.mean_,\n",
    "    \"var\": scaler.var_,\n",
    "    \"scale\": scaler.scale_\n",
    "})\n",
    "\n",
    "scaler_params.to_csv(os.path.join(output_dir, \"scaler_params.csv\"), index=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e4a6f6a-e12f-4f9f-a0a5-e83dfbe49467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imbalanced-learn not available: No module named 'imblearn'\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 — Augmentation helpers (SMOTE or noise fallback)\n",
    "use_smote = True\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except Exception as e:\n",
    "    print(\"imbalanced-learn not available:\", e)\n",
    "    use_smote = False\n",
    "\n",
    "def generate_with_noise(X_in, y_in, target_total_per_class, noise_scale=0.01):\n",
    "    X_arr = np.array(X_in)\n",
    "    y_arr = np.array(y_in)\n",
    "    classes = np.unique(y_arr)\n",
    "    gen_X = []\n",
    "    gen_y = []\n",
    "    rng = np.random.default_rng(42)\n",
    "    feature_stds = X_arr.std(axis=0)\n",
    "    for c in classes:\n",
    "        cls_idx = np.where(y_arr==c)[0]\n",
    "        if len(cls_idx)==0: continue\n",
    "        base = X_arr[cls_idx]\n",
    "        n = max(0, target_total_per_class - len(base))\n",
    "        if n <= 0: continue\n",
    "        sel_idx = rng.choice(len(base), size=n, replace=True)\n",
    "        sampled = base[sel_idx]\n",
    "        noise = rng.normal(0, noise_scale*feature_stds, size=sampled.shape)\n",
    "        aug = sampled + noise\n",
    "        gen_X.append(aug)\n",
    "        gen_y.append(np.full(n, c))\n",
    "    if len(gen_X)==0:\n",
    "        return np.empty((0,X_arr.shape[1])), np.empty((0,))\n",
    "    return np.vstack(gen_X), np.hstack(gen_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29c2e9f8-f873-48cb-a2d6-bcc525008250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 120\n",
      "Approx target per-class (rough): 400\n",
      "SMALL TEST MODE: using noise augmentation for a small expansion.\n",
      "Synthetic generated shape: (1080, 4) (1080,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 — Decide targets and generate synthetic data (safe defaults)\n",
    "n_train = X_train.shape[0]\n",
    "print(\"Train rows:\", n_train)\n",
    "# target_total_per_class = desired number of rows per class after augmentation\n",
    "approx_target_total = math.ceil((multiplier * n_train) / 3)\n",
    "print(\"Approx target per-class (rough):\", approx_target_total)\n",
    "\n",
    "if small_test_mode:\n",
    "    print(\"SMALL TEST MODE: using noise augmentation for a small expansion.\")\n",
    "    X_synth, y_synth = generate_with_noise(X_train, y_train, approx_target_total, noise_scale=0.02)\n",
    "else:\n",
    "    if use_smote:\n",
    "        print(\"WARNING: Chunked SMOTE generation code is heavy and must be run on Nautilus with sufficient RAM/CPU.\")\n",
    "        # naive SMOTE (may fail at extreme scale) — recommended to implement a chunking strategy in production\n",
    "        sm = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "        X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "        # For simplicity here we will repeat resample until rough multiplier achieved (not optimal)\n",
    "        repeats = multiplier // (len(X_res) // len(X_train) if len(X_train)>0 else 1)\n",
    "        X_synth = np.vstack([X_res[len(X_train):]] * max(1, repeats))\n",
    "        y_synth = np.hstack([y_res[len(X_train):]] * max(1, repeats))\n",
    "    else:\n",
    "        print(\"imbalanced-learn not available — falling back to noise augmentation at requested scale.\")\n",
    "        X_synth, y_synth = generate_with_noise(X_train, y_train, approx_target_total, noise_scale=0.02)\n",
    "\n",
    "print(\"Synthetic generated shape:\", X_synth.shape, y_synth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffcc809f-f30a-4529-8177-5a89573c973e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined dataset to: /home/jovyan/cloud_comp/module5/practices/project1/iris_smote_x10.csv Shape: (1200, 5)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 — Combine, shuffle, and save (Parquet recommended)\n",
    "if X_synth.size > 0:\n",
    "    X_comb = np.vstack([X_train, X_synth])\n",
    "    y_comb = np.hstack([y_train, y_synth])\n",
    "else:\n",
    "    X_comb = X_train\n",
    "    y_comb = y_train\n",
    "\n",
    "X_comb, y_comb = shuffle(X_comb, y_comb, random_state=42)\n",
    "df_comb = pd.DataFrame(X_comb, columns=[c.replace(\" (cm)\",\"\").strip() for c in iris.feature_names])\n",
    "df_comb[\"species\"] = y_comb.astype(int)\n",
    "\n",
    "out_csv = os.path.join(output_dir, f\"iris_smote_x{multiplier}.csv\")\n",
    "df_comb.to_csv(out_csv, index=False)\n",
    "print(\"Saved combined dataset to:\", out_csv, \"Shape:\", df_comb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb87cb7d-3835-40f3-8f71-670a7360c9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to PVC.\n"
     ]
    }
   ],
   "source": [
    "# Cell 11 — Save metadata and notes\n",
    "import json, time\n",
    "meta = {\n",
    "    \"multiplier\": multiplier,\n",
    "    \"combined_rows\": int(df_comb.shape[0]),\n",
    "    \"generated_rows\": int(X_synth.shape[0]) if X_synth.size else 0,\n",
    "    \"timestamp\": time.ctime(),\n",
    "    \"small_test_mode\": bool(small_test_mode)\n",
    "}\n",
    "with open(os.path.join(output_dir, \"generation_metadata.json\"), \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(\"Metadata saved to PVC.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11dfe991-bd79-4228-99be-49fc88558ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       0.82      0.90      0.86        10\n",
      "           2       0.89      0.80      0.84        10\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.90      0.90      0.90        30\n",
      "weighted avg       0.90      0.90      0.90        30\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/cloud_comp/module5/practices/project1/rf_model.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 10 — Quick validation model (RandomForest) using real held-out test set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "rf.fit(X_comb, y_comb)\n",
    "preds = rf.predict(X_test)\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, preds))\n",
    "print(classification_report(y_test, preds))\n",
    "joblib.dump(rf, os.path.join(output_dir, \"rf_model.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e64f7c43-04d9-45eb-bba2-5f3b27aa189d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 1 0 1 0 0 2 1 2 2 2 1 0 0 0 1 1 1 0 2 1 1 2 2 1 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the Random Forest model\n",
    "rf = joblib.load(\"/home/jovyan/cloud_comp/module5/practices/project1/rf_model.joblib\")\n",
    "\n",
    "# Use it for predictions\n",
    "predictions = rf.predict(X_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80f86867-9c2b-4e45-9cac-615def8fb2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote sample YAML to: /home/jovyan/cloud_comp/module5/practices/project1/iris_smote_job.yaml\n",
      "IMPORTANT: edit 'image' and 'claimName' before applying on Nautilus.\n"
     ]
    }
   ],
   "source": [
    "# Cell 12 — Generate a sample Kubernetes Job YAML (edit before apply)\n",
    "k8s_yaml = f'''\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: iris-smote-job\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: iris-smote\n",
    "        image: your-docker-repo/iris-smote:latest\n",
    "        command: [\"python\", \"/workspace/iris_smote_script.py\"]\n",
    "        volumeMounts:\n",
    "        - name: pvc-storage\n",
    "          mountPath: /mnt/data\n",
    "        resources:\n",
    "          limits:\n",
    "            cpu: \"8\"\n",
    "            memory: \"32Gi\"\n",
    "      restartPolicy: Never\n",
    "      volumes:\n",
    "      - name: pvc-storage\n",
    "        persistentVolumeClaim:\n",
    "          claimName: your-pvc-name\n",
    "  backoffLimit: 3\n",
    "'''\n",
    "yaml_path = os.path.join(output_dir, \"iris_smote_job.yaml\")\n",
    "with open(yaml_path, \"w\") as f:\n",
    "    f.write(k8s_yaml)\n",
    "print(\"Wrote sample YAML to:\", yaml_path)\n",
    "print(\"IMPORTANT: edit 'image' and 'claimName' before applying on Nautilus.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
